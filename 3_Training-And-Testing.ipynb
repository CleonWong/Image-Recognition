{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and Testing Our CNN\n",
    "\n",
    "In the first notebook, we downloaded and prepared the Fashion-MNIST data to be used as input for our CNN.\n",
    "\n",
    "In the second notebook, we designed the architecture of the CNN, defining the feedforward layers, the loss function and the optimisation (backpropagation) method.\n",
    "\n",
    "In this notebook, we will train our data on the `train_set` (with the help of our `validation_set`) and also test the accuracy of our model on the `test_set`.\n",
    "\n",
    "### Outline of this notebook\n",
    "\n",
    ">1. Preliminaries for training\n",
    "    - 1.1: `train` function\n",
    "    - 1.2: `test` function\n",
    "    - 1.3: Check accuracy of model before training\n",
    "<br>\n",
    "<br>\n",
    "2. Train the model\n",
    "<br>\n",
    "<br>\n",
    "3. Test the model\n",
    "<br>\n",
    "<br>\n",
    "4. Evaluation\n",
    "\n",
    "We first load the data and define the CNN. Refer to the first two notebooks for detailed walkthroughs of the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#### Load the data\n",
    "\n",
    "train_set = datasets.FashionMNIST(root = 'FashionMNIST_data', train = True, download = True)\n",
    "test_set = datasets.FashionMNIST(root = 'FashionMNIST_data', train = False, download = True)\n",
    "\n",
    "validation = 0.2\n",
    "\n",
    "training_size = len(train_set)\n",
    "indices = list(range(training_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(validation * training_size))\n",
    "train_index = indices[split:]\n",
    "validation_index = indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "validation_sampler = SubsetRandomSampler(validation_index)\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, ), (0.5, ))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, ), (0.5, ))])\n",
    "\n",
    "train_set = datasets.FashionMNIST(root='./FashionMNIST_data', train=True, download=False, transform=train_transform)\n",
    "test_set = datasets.FashionMNIST(root='./FashionMNIST_data', train=False, download=False, transform=test_transform)\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                           batch_size = batch_size,\n",
    "                                           sampler = train_sampler, \n",
    "                                           shuffle = False)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                                batch_size = batch_size, \n",
    "                                                sampler = validation_sampler, \n",
    "                                                shuffle = False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = True)\n",
    "\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "#### Design the CNN architecture\n",
    "\n",
    "num_filters = 10\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layer 1 (sees 28x28x1 image tensor)\n",
    "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size, padding = padding)\n",
    "        # Convolutional layer 2 (sees a 14x14x10 tensor)\n",
    "        self.conv2 = nn.Conv2d(num_filters, 20, kernel_size, padding = padding)\n",
    "        # Maxpooling layer of size 2x2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Fully-connected linear layer 1 (sees a 7x7x20 tensor -> 300)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 20, 300)\n",
    "        # Fully-connected linear layer 2 (300 -> 10)\n",
    "        self.fc2 = nn.Linear(300, 10)\n",
    "        # Dropout layer (p=0.2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Design sequence of convolutional and pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x))) # Outputs \n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten image input into a row vector to feed into the fully-connected layers\n",
    "        x = x.view(-1, 7 * 7 * 20)\n",
    "        # Add fully-connected layer 1 with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        # Add fully-connected layer 2 with dropout\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        return x\n",
    "    \n",
    "model = CNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Prelimiaries for training\n",
    "\n",
    "#### 1.1: Define the `train` function\n",
    "\n",
    "We start by defining a `train` function that will be called when we train our CNN. We will train over a number of `epochs`, which is the number of times that we will cycle through the entire `train_set`.\n",
    "\n",
    "The sequence of steps that is performed for each epoch is:\n",
    ">1. Train with the `train_set`:\n",
    "    - Step 1: Zero's the gradients to prepare for feedforward\n",
    "    - Step 2: Feedforward the input image through the network\n",
    "    - Step 3: Compute the loss relative to `label`\n",
    "    - Step 4: Propagate gradients back into the network's parameters (i.e. calculate the gradient with of the loss function with respect to each parameter)\n",
    "    - Step 5: Optimise / update the parameters\n",
    "    - Step 6: Sum the `training_loss`\n",
    "<br>\n",
    "<br>\n",
    "2. Evaluate with the `validation_set`:\n",
    "    - Step 1: Feedforward the image through the trained model\n",
    "    - Step 2: Compute the loss\n",
    "    - Step 3: Sum the `validation_loss`\n",
    "<br>\n",
    "<br>\n",
    "3. After each epoch, we compute the average `training_loss` and `validation_loss` per batch and store it in the lists `training_loss_mem` and `validation_loss_mem` respectively (we will use these lists to visualise the losses). Then, we print the losses.\n",
    "\n",
    "Additionally, from one epoch to another, we keep track of the average validation loss. We save the model as `CNN_FashionMNIST.pt` each time our model attains a new minimum average validation loss for an epoch.\n",
    "\n",
    "Also note that we use `torch.cuda.is_available()` to check if CUDA is available for training on the machine that we are using. If it is available (i.e. returns `True`), we can transfer the image and label tensors to the machine's GPU, else the training will be done on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define train function\n",
    "\n",
    "def train(train_loader, validation_loader, n_epochs):\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    \n",
    "    # Set minimum validation loss as infinity.\n",
    "    # This is used to check and save the model.\n",
    "    validation_loss_min = np.Inf\n",
    "    \n",
    "    # Create lists to store training and validation losses after each epoch\n",
    "    training_loss_mem = []\n",
    "    validation_loss_mem = []\n",
    "    \n",
    "    # Iterate over train_set for n_epochs times\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Keep track of training and validation loss\n",
    "        total_training_loss = 0\n",
    "        total_validation_loss = 0\n",
    "        \n",
    "        # ----- Train the model ------\n",
    "        model.train()\n",
    "        \n",
    "        for image, label in train_loader:\n",
    "            # Move tensors to GPU if CUDA is available\n",
    "            if use_cuda:\n",
    "                image, label = image.cuda(), label.cuda()\n",
    "            # Step 1: Zero the gradients\n",
    "            optimiser.zero_grad()\n",
    "            # Step 2: Feedforward the input image\n",
    "            output = model(image)\n",
    "            # Step 3: Compute the loss\n",
    "            loss = criterion(output, label)\n",
    "            # Step 4: Backpropagation\n",
    "            loss.backward()\n",
    "            # Step 5: Update parameters\n",
    "            optimiser.step()\n",
    "            # Step 6: Update training loss\n",
    "            total_training_loss += loss.item()\n",
    "\n",
    "        # ----- Validate the model ------\n",
    "        \n",
    "        # Turn off gradients for validation\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for image, label in validation_loader:\n",
    "                # Move tensors to GPU if CUDA is available\n",
    "                if use_cuda:\n",
    "                    image, label = image.cuda(), label.cuda()\n",
    "                # Step 1: Feedforward: Compute predicted outputs after\n",
    "                # training on the train_set above\n",
    "                output = model(image)\n",
    "                # Step 2: Compute loss\n",
    "                loss = criterion(output, label)\n",
    "                # Step 3: Update validation loss\n",
    "                total_validation_loss += loss.item()\n",
    "\n",
    "        # Compute average losses in this epoch\n",
    "        avg_training_loss = total_training_loss / len(train_loader.sampler)\n",
    "        avg_validation_loss = total_validation_loss / len(validation_loader.sampler)\n",
    "        training_loss_mem.append(avg_training_loss)\n",
    "        validation_loss_mem.append(avg_validation_loss)\n",
    "        \n",
    "        # Print average training and validation loss in this epoch\n",
    "        print(\"Epoch: {} out of {}\".format(epoch+1, n_epochs))\n",
    "        print(\"Average training loss: {:.6f}\".format(avg_training_loss))\n",
    "        print(\"Average validation loss: {:.6f}\".format(avg_validation_loss))\n",
    "        \n",
    "        # ------ Save model if validation loss has decreased ------\n",
    "        if avg_validation_loss <= validation_loss_min:\n",
    "            print(\"Validation loss decreased from {} in the previous epoch to {} in this epoch.\".format(validation_loss_min, avg_validation_loss))\n",
    "            print()\n",
    "            torch.save(model.state_dict(), 'CNN_FashionMNIST.pt')\n",
    "            validation_loss_min = avg_validation_loss\n",
    "        else:\n",
    "            print(\"Validation loss did not decrease in this epoch.\")\n",
    "            print()\n",
    "        \n",
    "    return training_loss_mem, validation_loss_mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Define the `test` function\n",
    "\n",
    "Below, we define the `test` function that we will call when testing our model on unseen images in the `test_set`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define test function\n",
    "\n",
    "def test(test_loader, saved_NN = None):\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    \n",
    "    # Contruct dictionaries to count the number of correct and\n",
    "    # incorrect predictions for each class\n",
    "    correct_dict = dict(zip(classes, [0]*len(classes)))\n",
    "    incorrect_dict = dict(zip(classes, [0]*len(classes)))\n",
    "    \n",
    "    class_correctcount = list(0 for i in range(10))\n",
    "    class_totalcount = list(0 for i in range(10))\n",
    "    \n",
    "    # Track test loss\n",
    "    total_test_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    # If there is a saved model, load the model\n",
    "    if saved_NN:\n",
    "        model.load_state_dict(torch.load(saved_NN))\n",
    "    \n",
    "    # ------ Test the model ------\n",
    "    \n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for image, label in test_loader:\n",
    "            # Move tensors to GPU if CUDA is available\n",
    "            if use_cuda:\n",
    "                image, label = image.cuda(), label.cuda()\n",
    "            # Step 1: Feedforward\n",
    "            output = model(image)\n",
    "            # Step 2: Compute loss\n",
    "            loss = criterion(output, label)\n",
    "            # Step 3: Update test loss\n",
    "            total_test_loss += loss.item()\n",
    "            # Step 4: Convert output values to log probabilities using logSoftmax\n",
    "            log_probs = F.log_softmax(output, dim = 1)\n",
    "            # Step 5: Convert log probabilities to probabilities\n",
    "            probs = torch.exp(log_probs)\n",
    "            # Step 6: Determine the predicted class\n",
    "            # Note - `predicted_class_idx` is a tensor that contains the indices\n",
    "            # of the predicted class of each image\n",
    "            _, predicted_class_idx = probs.topk(1, dim=1)\n",
    "            # Step 7: Compare prediction to the true answers in `label`\n",
    "            # Note - `correct_tensor` is a tensor of booleans\n",
    "            correct_tensor = predicted_class_idx.eq(label.data.view_as(predicted_class_idx))\n",
    "            # Note - `correct_np` is a numpy.ndarray of booleans\n",
    "            correct_np = np.squeeze(correct_tensor.numpy()) if not use_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
    "            # Step 8: Compute test accuracy for each class\n",
    "            for i in range(batch_size):\n",
    "                # Note - `answer` is a tensor\n",
    "                # Note - `answer` is the true answer taken from `label`.\n",
    "                #         This provides us with the index that we should at to\n",
    "                #         in `class_correctcount` and `class_totalcount`\n",
    "                answer = label[i]\n",
    "                # Note - `correct_np[i].item()` returns the boolean at index i\n",
    "                # Note - This step counts the number of correct predictions for each index (i.e. each class)\n",
    "                class_correctcount[answer] += correct_np[i].item()\n",
    "                # Note - This counts the total number of predictions made for each index (i.e. each class)\n",
    "                class_totalcount[answer] += 1\n",
    "\n",
    "                \n",
    "        # Compute average test loss per batch\n",
    "        avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "        print(\"Average test loss per batch: {:.6f}\".format(avg_test_loss))\n",
    "        print()\n",
    "        \n",
    "        # Compute test accuracy across all batches in this test epoch\n",
    "        print(\"Average test accuracy per batch: {:.2f}% ({:0d}/{:0d})\".format((np.sum(class_correctcount)/np.sum(class_totalcount))*100,\n",
    "                                                                      np.sum(class_correctcount),\n",
    "                                                                      np.sum(class_totalcount)))\n",
    "        print()\n",
    "        \n",
    "        # Compute test accuracy per class\n",
    "        for i in range(len(classes)):\n",
    "            if class_totalcount[i] > 0:\n",
    "                print(\"Test accuracy of {}: {:.2f}% ({:0d}/{:0d})\".format(classes[i],\n",
    "                                                                   (class_correctcount[i]/class_totalcount[i])*100,\n",
    "                                                                   class_correctcount[i],\n",
    "                                                                   class_totalcount[i]))\n",
    "            else:\n",
    "                print(\"Test accuracy of {}: N/A (no training examples)\".format(classes[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3: Check accuracy of model before training\n",
    "\n",
    "Before we proceed to train our model, it is interesting to check out the accuracy of our CNN. Since the parameters of our CNN are randomly generated, we should expect the accuracy to be around 1/10 (i.e. 10%), implying that the prediction of our untrained model is as good as a random guess!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss per batch: 0.115207\n",
      "\n",
      "Average test accuracy per batch: 10.06% (1006/10000)\n",
      "\n",
      "Test accuracy of T-shirt/top: 0.00% (0/1000)\n",
      "Test accuracy of Trouser: 1.50% (15/1000)\n",
      "Test accuracy of Pullover: 0.00% (0/1000)\n",
      "Test accuracy of Dress: 0.00% (0/1000)\n",
      "Test accuracy of Coat: 0.00% (0/1000)\n",
      "Test accuracy of Sandal: 0.00% (0/1000)\n",
      "Test accuracy of Shirt: 0.00% (0/1000)\n",
      "Test accuracy of Sneaker: 99.10% (991/1000)\n",
      "Test accuracy of Bag: 0.00% (0/1000)\n",
      "Test accuracy of Ankle boot: 0.00% (0/1000)\n"
     ]
    }
   ],
   "source": [
    "test(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our results above, we see that indeed, the average accuracy of our model's prediction per batch is about 10%! There are even some classes where our model got completely wrong. Bearing this results in mind, we shall now train our model.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Train the model\n",
    "\n",
    "Remember to look at how the training and validation loss decreases over time; if the validation loss ever increases it indicates possible overfitting. (In fact, in the below example, we could have stopped around epoch XXX or so!)\n",
    "\n",
    "We will train it over 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 out of 100\n",
      "Average training loss: 0.058059\n",
      "Average validation loss: 0.028904\n",
      "Validation loss decreased from inf in the previous epoch to 0.028904018541177114 in this epoch.\n",
      "\n",
      "Epoch: 2 out of 100\n",
      "Average training loss: 0.043815\n",
      "Average validation loss: 0.025610\n",
      "Validation loss decreased from 0.028904018541177114 in the previous epoch to 0.025609595754494268 in this epoch.\n",
      "\n",
      "Epoch: 3 out of 100\n",
      "Average training loss: 0.041043\n",
      "Average validation loss: 0.021918\n",
      "Validation loss decreased from 0.025609595754494268 in the previous epoch to 0.021917994622762005 in this epoch.\n",
      "\n",
      "Epoch: 4 out of 100\n",
      "Average training loss: 0.039791\n",
      "Average validation loss: 0.019975\n",
      "Validation loss decreased from 0.021917994622762005 in the previous epoch to 0.019974799756892026 in this epoch.\n",
      "\n",
      "Epoch: 5 out of 100\n",
      "Average training loss: 0.038761\n",
      "Average validation loss: 0.019186\n",
      "Validation loss decreased from 0.019974799756892026 in the previous epoch to 0.019186465597401062 in this epoch.\n",
      "\n",
      "Epoch: 6 out of 100\n",
      "Average training loss: 0.037968\n",
      "Average validation loss: 0.018468\n",
      "Validation loss decreased from 0.019186465597401062 in the previous epoch to 0.018467628294912476 in this epoch.\n",
      "\n",
      "Epoch: 7 out of 100\n",
      "Average training loss: 0.036997\n",
      "Average validation loss: 0.017555\n",
      "Validation loss decreased from 0.018467628294912476 in the previous epoch to 0.017554552920162676 in this epoch.\n",
      "\n",
      "Epoch: 8 out of 100\n",
      "Average training loss: 0.036459\n",
      "Average validation loss: 0.017490\n",
      "Validation loss decreased from 0.017554552920162676 in the previous epoch to 0.01749036899084846 in this epoch.\n",
      "\n",
      "Epoch: 9 out of 100\n",
      "Average training loss: 0.036344\n",
      "Average validation loss: 0.017228\n",
      "Validation loss decreased from 0.01749036899084846 in the previous epoch to 0.01722758209519088 in this epoch.\n",
      "\n",
      "Epoch: 10 out of 100\n",
      "Average training loss: 0.035421\n",
      "Average validation loss: 0.016646\n",
      "Validation loss decreased from 0.01722758209519088 in the previous epoch to 0.016646233594976367 in this epoch.\n",
      "\n",
      "Epoch: 11 out of 100\n",
      "Average training loss: 0.035004\n",
      "Average validation loss: 0.015996\n",
      "Validation loss decreased from 0.016646233594976367 in the previous epoch to 0.015995619367808103 in this epoch.\n",
      "\n",
      "Epoch: 12 out of 100\n",
      "Average training loss: 0.035175\n",
      "Average validation loss: 0.017295\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 13 out of 100\n",
      "Average training loss: 0.034453\n",
      "Average validation loss: 0.016225\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 14 out of 100\n",
      "Average training loss: 0.034153\n",
      "Average validation loss: 0.015580\n",
      "Validation loss decreased from 0.015995619367808103 in the previous epoch to 0.015580276572455962 in this epoch.\n",
      "\n",
      "Epoch: 15 out of 100\n",
      "Average training loss: 0.033838\n",
      "Average validation loss: 0.016772\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 16 out of 100\n",
      "Average training loss: 0.033655\n",
      "Average validation loss: 0.015669\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 17 out of 100\n",
      "Average training loss: 0.033181\n",
      "Average validation loss: 0.015728\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 18 out of 100\n",
      "Average training loss: 0.033103\n",
      "Average validation loss: 0.015023\n",
      "Validation loss decreased from 0.015580276572455962 in the previous epoch to 0.015023338115774096 in this epoch.\n",
      "\n",
      "Epoch: 19 out of 100\n",
      "Average training loss: 0.032720\n",
      "Average validation loss: 0.015711\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 20 out of 100\n",
      "Average training loss: 0.032453\n",
      "Average validation loss: 0.015282\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 21 out of 100\n",
      "Average training loss: 0.031994\n",
      "Average validation loss: 0.014784\n",
      "Validation loss decreased from 0.015023338115774096 in the previous epoch to 0.014783806714384506 in this epoch.\n",
      "\n",
      "Epoch: 22 out of 100\n",
      "Average training loss: 0.032088\n",
      "Average validation loss: 0.014847\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 23 out of 100\n",
      "Average training loss: 0.031570\n",
      "Average validation loss: 0.015192\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 24 out of 100\n",
      "Average training loss: 0.031538\n",
      "Average validation loss: 0.015296\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 25 out of 100\n",
      "Average training loss: 0.031188\n",
      "Average validation loss: 0.014876\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 26 out of 100\n",
      "Average training loss: 0.031126\n",
      "Average validation loss: 0.015037\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 27 out of 100\n",
      "Average training loss: 0.031104\n",
      "Average validation loss: 0.014974\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 28 out of 100\n",
      "Average training loss: 0.030559\n",
      "Average validation loss: 0.014318\n",
      "Validation loss decreased from 0.014783806714384506 in the previous epoch to 0.014318277858275299 in this epoch.\n",
      "\n",
      "Epoch: 29 out of 100\n",
      "Average training loss: 0.030687\n",
      "Average validation loss: 0.015200\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 30 out of 100\n",
      "Average training loss: 0.030374\n",
      "Average validation loss: 0.015154\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 31 out of 100\n",
      "Average training loss: 0.030165\n",
      "Average validation loss: 0.015455\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 32 out of 100\n",
      "Average training loss: 0.030062\n",
      "Average validation loss: 0.015219\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 33 out of 100\n",
      "Average training loss: 0.029808\n",
      "Average validation loss: 0.014876\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 34 out of 100\n",
      "Average training loss: 0.030100\n",
      "Average validation loss: 0.015130\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 35 out of 100\n",
      "Average training loss: 0.029429\n",
      "Average validation loss: 0.015390\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 36 out of 100\n",
      "Average training loss: 0.029245\n",
      "Average validation loss: 0.015402\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 37 out of 100\n",
      "Average training loss: 0.029394\n",
      "Average validation loss: 0.015562\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 38 out of 100\n",
      "Average training loss: 0.029070\n",
      "Average validation loss: 0.015683\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 39 out of 100\n",
      "Average training loss: 0.028654\n",
      "Average validation loss: 0.014773\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 40 out of 100\n",
      "Average training loss: 0.028529\n",
      "Average validation loss: 0.014216\n",
      "Validation loss decreased from 0.014318277858275299 in the previous epoch to 0.01421579905692488 in this epoch.\n",
      "\n",
      "Epoch: 41 out of 100\n",
      "Average training loss: 0.028622\n",
      "Average validation loss: 0.014623\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 42 out of 100\n",
      "Average training loss: 0.028451\n",
      "Average validation loss: 0.015205\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 43 out of 100\n",
      "Average training loss: 0.028495\n",
      "Average validation loss: 0.015813\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 44 out of 100\n",
      "Average training loss: 0.028141\n",
      "Average validation loss: 0.015100\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 45 out of 100\n",
      "Average training loss: 0.027915\n",
      "Average validation loss: 0.015344\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 46 out of 100\n",
      "Average training loss: 0.027920\n",
      "Average validation loss: 0.015362\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 47 out of 100\n",
      "Average training loss: 0.027852\n",
      "Average validation loss: 0.016076\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 48 out of 100\n",
      "Average training loss: 0.027820\n",
      "Average validation loss: 0.015224\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 49 out of 100\n",
      "Average training loss: 0.027583\n",
      "Average validation loss: 0.015940\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 50 out of 100\n",
      "Average training loss: 0.027942\n",
      "Average validation loss: 0.015529\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 51 out of 100\n",
      "Average training loss: 0.027738\n",
      "Average validation loss: 0.015966\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 52 out of 100\n",
      "Average training loss: 0.027565\n",
      "Average validation loss: 0.015776\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 53 out of 100\n",
      "Average training loss: 0.027614\n",
      "Average validation loss: 0.015846\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 out of 100\n",
      "Average training loss: 0.027221\n",
      "Average validation loss: 0.015492\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 55 out of 100\n",
      "Average training loss: 0.027273\n",
      "Average validation loss: 0.016277\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 56 out of 100\n",
      "Average training loss: 0.026815\n",
      "Average validation loss: 0.016185\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 57 out of 100\n",
      "Average training loss: 0.027006\n",
      "Average validation loss: 0.016369\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 58 out of 100\n",
      "Average training loss: 0.026938\n",
      "Average validation loss: 0.016283\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 59 out of 100\n",
      "Average training loss: 0.026883\n",
      "Average validation loss: 0.016755\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 60 out of 100\n",
      "Average training loss: 0.026999\n",
      "Average validation loss: 0.016453\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 61 out of 100\n",
      "Average training loss: 0.026455\n",
      "Average validation loss: 0.016240\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 62 out of 100\n",
      "Average training loss: 0.026788\n",
      "Average validation loss: 0.016309\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 63 out of 100\n",
      "Average training loss: 0.026932\n",
      "Average validation loss: 0.016676\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 64 out of 100\n",
      "Average training loss: 0.026384\n",
      "Average validation loss: 0.016550\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 65 out of 100\n",
      "Average training loss: 0.026500\n",
      "Average validation loss: 0.016653\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 66 out of 100\n",
      "Average training loss: 0.026366\n",
      "Average validation loss: 0.016935\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 67 out of 100\n",
      "Average training loss: 0.026209\n",
      "Average validation loss: 0.016677\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 68 out of 100\n",
      "Average training loss: 0.026040\n",
      "Average validation loss: 0.017495\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 69 out of 100\n",
      "Average training loss: 0.026310\n",
      "Average validation loss: 0.017372\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 70 out of 100\n",
      "Average training loss: 0.026358\n",
      "Average validation loss: 0.017430\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 71 out of 100\n",
      "Average training loss: 0.026035\n",
      "Average validation loss: 0.018395\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 72 out of 100\n",
      "Average training loss: 0.026170\n",
      "Average validation loss: 0.017603\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 73 out of 100\n",
      "Average training loss: 0.026029\n",
      "Average validation loss: 0.017576\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 74 out of 100\n",
      "Average training loss: 0.025946\n",
      "Average validation loss: 0.018047\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 75 out of 100\n",
      "Average training loss: 0.026040\n",
      "Average validation loss: 0.018323\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 76 out of 100\n",
      "Average training loss: 0.025318\n",
      "Average validation loss: 0.017182\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 77 out of 100\n",
      "Average training loss: 0.025881\n",
      "Average validation loss: 0.017670\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 78 out of 100\n",
      "Average training loss: 0.025788\n",
      "Average validation loss: 0.017851\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 79 out of 100\n",
      "Average training loss: 0.025496\n",
      "Average validation loss: 0.017883\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 80 out of 100\n",
      "Average training loss: 0.025783\n",
      "Average validation loss: 0.017818\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 81 out of 100\n",
      "Average training loss: 0.025179\n",
      "Average validation loss: 0.018000\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 82 out of 100\n",
      "Average training loss: 0.025634\n",
      "Average validation loss: 0.017664\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 83 out of 100\n",
      "Average training loss: 0.025690\n",
      "Average validation loss: 0.018353\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 84 out of 100\n",
      "Average training loss: 0.025814\n",
      "Average validation loss: 0.018638\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 85 out of 100\n",
      "Average training loss: 0.025569\n",
      "Average validation loss: 0.018134\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 86 out of 100\n",
      "Average training loss: 0.025816\n",
      "Average validation loss: 0.018868\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 87 out of 100\n",
      "Average training loss: 0.025807\n",
      "Average validation loss: 0.018477\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 88 out of 100\n",
      "Average training loss: 0.025434\n",
      "Average validation loss: 0.018925\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 89 out of 100\n",
      "Average training loss: 0.025755\n",
      "Average validation loss: 0.017972\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 90 out of 100\n",
      "Average training loss: 0.025377\n",
      "Average validation loss: 0.018762\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 91 out of 100\n",
      "Average training loss: 0.025425\n",
      "Average validation loss: 0.018414\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 92 out of 100\n",
      "Average training loss: 0.025307\n",
      "Average validation loss: 0.018331\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 93 out of 100\n",
      "Average training loss: 0.025277\n",
      "Average validation loss: 0.018169\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 94 out of 100\n",
      "Average training loss: 0.025254\n",
      "Average validation loss: 0.018568\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 95 out of 100\n",
      "Average training loss: 0.025154\n",
      "Average validation loss: 0.018407\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 96 out of 100\n",
      "Average training loss: 0.025086\n",
      "Average validation loss: 0.018935\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 97 out of 100\n",
      "Average training loss: 0.025319\n",
      "Average validation loss: 0.019397\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 98 out of 100\n",
      "Average training loss: 0.025270\n",
      "Average validation loss: 0.018387\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 99 out of 100\n",
      "Average training loss: 0.025142\n",
      "Average validation loss: 0.019004\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 100 out of 100\n",
      "Average training loss: 0.025186\n",
      "Average validation loss: 0.019236\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_loss_mem, validation_loss_mem = train(train_loader, validation_loader, n_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x116e8f2b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD7CAYAAABkO19ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.plot(training_loss_mem, label='Training loss')\n",
    "plt.plot(validation_loss_mem, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have intentionally trained the network over 100 epochs to illustrate the problem of overfitting our model with the training data. As seen from the blue line, the training loss will always decrease as we increase the number of training epochs. However, when we test our trained model on the validation set after each epoch, we notice that the validation loss will decrease till a certain minimum point before increasing again. In our case, the turning point is around epochs 20-40. After those epochs, the validation loss started to increase, indicating that we have overfitted our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Test the model\n",
    "\n",
    "Now that we have trained our model, we can test our model and hopefully get a better prediction accuracy. We test our model by importing the saved model from our training, `CNN_FashionMNIST.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss per batch: 0.015289\n",
      "\n",
      "Average test accuracy per batch: 91.64% (9164/10000)\n",
      "\n",
      "Test accuracy of T-shirt/top: 87.20% (872/1000)\n",
      "Test accuracy of Trouser: 98.50% (985/1000)\n",
      "Test accuracy of Pullover: 88.60% (886/1000)\n",
      "Test accuracy of Dress: 93.00% (930/1000)\n",
      "Test accuracy of Coat: 83.20% (832/1000)\n",
      "Test accuracy of Sandal: 98.50% (985/1000)\n",
      "Test accuracy of Shirt: 76.20% (762/1000)\n",
      "Test accuracy of Sneaker: 97.60% (976/1000)\n",
      "Test accuracy of Bag: 98.20% (982/1000)\n",
      "Test accuracy of Ankle boot: 95.40% (954/1000)\n"
     ]
    }
   ],
   "source": [
    "test(test_loader, saved_NN = 'CNN_FashionMNIST.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
