{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and Testing Our CNN\n",
    "\n",
    "In the first notebook, we downloaded and prepared the Fashion-MNIST data to be used as input for our CNN.\n",
    "\n",
    "In the second notebook, we designed the architecture of the CNN, defining the feedforward layers, the loss function and the optimisation (backpropagation) method.\n",
    "\n",
    "In this notebook, we will train our data on the `train_set` (with the help of our `validation_set`) and also test the accuracy of our model on the `test_set`.\n",
    "\n",
    "### Outline of this notebook\n",
    "\n",
    ">1. Preliminaries for training\n",
    "    - 1.1: `train` function\n",
    "    - 1.2: `test` function\n",
    "    - 1.3: Check accuracy of model before training\n",
    "<br>\n",
    "<br>\n",
    "2. Train the model\n",
    "<br>\n",
    "<br>\n",
    "3. Test the model\n",
    "\n",
    "We first load the data and define the CNN. Refer to the first two notebooks for detailed walkthroughs of the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#### Load the data\n",
    "\n",
    "train_set = datasets.FashionMNIST(root = 'FashionMNIST_data', train = True, download = True)\n",
    "test_set = datasets.FashionMNIST(root = 'FashionMNIST_data', train = False, download = True)\n",
    "\n",
    "validation = 0.2\n",
    "\n",
    "training_size = len(train_set)\n",
    "indices = list(range(training_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(validation * training_size))\n",
    "train_index = indices[:split]\n",
    "validation_index = indices[split:]\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "validation_sampler = SubsetRandomSampler(validation_index)\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, ), (0.5, ))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, ), (0.5, ))])\n",
    "\n",
    "train_set = datasets.FashionMNIST(root='./FashionMNIST_data', train=True, download=False, transform=train_transform)\n",
    "test_set = datasets.FashionMNIST(root='./FashionMNIST_data', train=False, download=False, transform=test_transform)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                           batch_size = batch_size,\n",
    "                                           sampler = train_sampler, \n",
    "                                           shuffle = False)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                                batch_size = batch_size, \n",
    "                                                sampler = validation_sampler, \n",
    "                                                shuffle = False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = True)\n",
    "\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "#### Design the CNN architecture\n",
    "\n",
    "num_filters = 10\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layer 1 (sees 28x28x1 image tensor)\n",
    "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size, padding = padding)\n",
    "        # Convolutional layer 2 (sees a 14x14x10 tensor)\n",
    "        self.conv2 = nn.Conv2d(num_filters, 20, kernel_size, padding = padding)\n",
    "        # Maxpooling layer of size 2x2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Fully-connected linear layer 1 (sees a 7x7x20 tensor -> 300)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 20, 300)\n",
    "        # Fully-connected linear layer 2 (300 -> 10)\n",
    "        self.fc2 = nn.Linear(300, 10)\n",
    "        # Dropout layer (p=0.2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Design sequence of convolutional and pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x))) # Outputs \n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten image input into a row vector to feed into the fully-connected layers\n",
    "        x = x.view(-1, 7 * 7 * 20)\n",
    "        # Add fully-connected layer 1 with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        # Add fully-connected layer 2 with dropout\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        return x\n",
    "    \n",
    "model = CNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Prelimiaries for training\n",
    "\n",
    "#### 1.1: Define the `train` function\n",
    "\n",
    "We start by defining a `train` function that will be called when we train our CNN. We will train over a number of `epochs`, which is the number of times that we will cycle through the entire `train_set`.\n",
    "\n",
    "The sequence of steps that is performed for each epoch is:\n",
    ">1. Train with the `train_set`:\n",
    "    - Step 1: Zero's the gradients to prepare for feedforward\n",
    "    - Step 2: Feedforward the input image through the network\n",
    "    - Step 3: Compute the loss relative to `label`\n",
    "    - Step 4: Propagate gradients back into the network's parameters (i.e. calculate the gradient with of the loss function with respect to each parameter)\n",
    "    - Step 5: Optimise / update the parameters\n",
    "    - Step 6: Sum the `training_loss`\n",
    "<br>\n",
    "<br>\n",
    "2. Evaluate with the `validation_set`:\n",
    "    - Step 1: Feedforward the image through the trained model\n",
    "    - Step 2: Compute the loss\n",
    "    - Step 3: Sum the `validation_loss`\n",
    "<br>\n",
    "<br>\n",
    "3. After each epoch, we compute the average `training_loss` and `validation_loss` per batch and store it in the lists `training_loss_mem` and `validation_loss_mem` respectively (we will use these lists to visualise the losses). Then, we print the losses.\n",
    "\n",
    "Additionally, from one epoch to another, we keep track of the average validation loss. We save the model as `CNN_FashionMNIST.pt` each time our model attains a new minimum average validation loss for an epoch.\n",
    "\n",
    "Also note that we use `torch.cuda.is_available()` to check if CUDA is available for training on the machine that we are using. If it is available (i.e. returns `True`), we can transfer the image and label tensors to the machine's GPU, else the training will be done on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define train function\n",
    "\n",
    "def train(train_loader, validation_loader, n_epochs):\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    \n",
    "    # Set minimum validation loss as infinity.\n",
    "    # This is used to check and save the model.\n",
    "    validation_loss_min = np.Inf\n",
    "    \n",
    "    # Create lists to store training and validation losses after each epoch\n",
    "    training_loss_mem = []\n",
    "    validation_loss_mem = []\n",
    "    \n",
    "    # Iterate over train_set for n_epochs times\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Keep track of training and validation loss\n",
    "        total_training_loss = 0\n",
    "        total_validation_loss = 0\n",
    "        \n",
    "        # ----- Train the model ------\n",
    "        model.train()\n",
    "        \n",
    "        for image, label in train_loader:\n",
    "            # Move tensors to GPU if CUDA is available\n",
    "            if use_cuda:\n",
    "                image, label = image.cuda(), label.cuda()\n",
    "            # Step 1: Zero the gradients\n",
    "            optimiser.zero_grad()\n",
    "            # Step 2: Feedforward the input image\n",
    "            output = model(image)\n",
    "            # Step 3: Compute the loss\n",
    "            loss = criterion(output, label)\n",
    "            # Step 4: Backpropagation\n",
    "            loss.backward()\n",
    "            # Step 5: Update parameters\n",
    "            optimiser.step()\n",
    "            # Step 6: Update training loss\n",
    "            total_training_loss += loss.item()\n",
    "\n",
    "        # ----- Validate the model ------\n",
    "        \n",
    "        # Turn off gradients for validation\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for image, label in validation_loader:\n",
    "                # Move tensors to GPU if CUDA is available\n",
    "                if use_cuda:\n",
    "                    image, label = image.cuda(), label.cuda()\n",
    "                # Step 1: Feedforward: Compute predicted outputs after\n",
    "                # training on the train_set above\n",
    "                output = model(image)\n",
    "                # Step 2: Compute loss\n",
    "                loss = criterion(output, label)\n",
    "                # Step 3: Update validation loss\n",
    "                total_validation_loss += loss.item()\n",
    "\n",
    "        # Compute average losses in this epoch\n",
    "        avg_training_loss = total_training_loss / len(train_loader.sampler)\n",
    "        avg_validation_loss = total_validation_loss / len(validation_loader.sampler)\n",
    "        training_loss_mem.append(avg_training_loss)\n",
    "        validation_loss_mem.append(avg_validation_loss)\n",
    "        \n",
    "        # Print average training and validation loss in this epoch\n",
    "        print(\"Epoch: {} out of {}\".format(epoch+1, n_epochs))\n",
    "        print(\"Average training loss: {:.6f}\".format(avg_training_loss))\n",
    "        print(\"Average validation loss: {:.6f}\".format(avg_validation_loss))\n",
    "        \n",
    "        # ------ Save model if validation loss has decreased ------\n",
    "        if avg_validation_loss <= validation_loss_min:\n",
    "            print(\"Validation loss decreased from {} in the previous epoch to {} in this epoch.\".format(validation_loss_min, avg_validation_loss))\n",
    "            print()\n",
    "            torch.save(model.state_dict(), 'CNN_FashionMNIST.pt')\n",
    "            validation_loss_min = avg_validation_loss\n",
    "        else:\n",
    "            print(\"Validation loss did not decrease in this epoch.\")\n",
    "            print()\n",
    "        \n",
    "    return training_loss_mem, validation_loss_mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Define the `test` function\n",
    "\n",
    "Below, we define the `test` function that we will call when testing our model on unseen images in the `test_set`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define test function\n",
    "\n",
    "def test(test_loader, saved_NN = None):\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    \n",
    "    # Contruct dictionaries to count the number of correct and\n",
    "    # incorrect predictions for each class\n",
    "    correct_dict = dict(zip(classes, [0]*len(classes)))\n",
    "    incorrect_dict = dict(zip(classes, [0]*len(classes)))\n",
    "    \n",
    "    class_correctcount = list(0 for i in range(10))\n",
    "    class_totalcount = list(0 for i in range(10))\n",
    "    \n",
    "    # Track test loss\n",
    "    total_test_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    # If there is a saved model, load the model\n",
    "    if saved_NN:\n",
    "        model.load_state_dict(torch.load(saved_NN))\n",
    "    \n",
    "    # ------ Test the model ------\n",
    "    \n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for image, label in test_loader:\n",
    "            # Move tensors to GPU if CUDA is available\n",
    "            if use_cuda:\n",
    "                image, label = image.cuda(), label.cuda()\n",
    "            # Step 1: Feedforward\n",
    "            output = model(image)\n",
    "            # Step 2: Compute loss\n",
    "            loss = criterion(output, label)\n",
    "            # Step 3: Update test loss\n",
    "            total_test_loss += loss.item()\n",
    "            # Step 4: Convert output values to log probabilities using logSoftmax\n",
    "            log_probs = F.log_softmax(output, dim = 1)\n",
    "            # Step 5: Convert log probabilities to probabilities\n",
    "            probs = torch.exp(log_probs)\n",
    "            # Step 6: Determine the predicted class\n",
    "            # Note - `predicted_class_idx` is a tensor that contains the indices\n",
    "            # of the predicted class of each image\n",
    "            _, predicted_class_idx = probs.topk(1, dim=1)\n",
    "            # Step 7: Compare prediction to the true answers in `label`\n",
    "            # Note - `correct_tensor` is a tensor of booleans\n",
    "            correct_tensor = predicted_class_idx.eq(label.data.view_as(predicted_class_idx))\n",
    "            # Note - `correct_np` is a numpy.ndarray of booleans\n",
    "            correct_np = np.squeeze(correct_tensor.numpy()) if not use_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
    "            # Step 8: Compute test accuracy for each class\n",
    "            for i in range(batch_size):\n",
    "                # Note - `answer` is a tensor\n",
    "                # Note - `answer` is the true answer taken from `label`.\n",
    "                #         This provides us with the index that we should at to\n",
    "                #         in `class_correctcount` and `class_totalcount`\n",
    "                answer = label[i]\n",
    "                # Note - `correct_np[i].item()` returns the boolean at index i\n",
    "                # Note - This step counts the number of correct predictions for each index (i.e. each class)\n",
    "                class_correctcount[answer] += correct_np[i].item()\n",
    "                # Note - This counts the total number of predictions made for each index (i.e. each class)\n",
    "                class_totalcount[answer] += 1\n",
    "\n",
    "                \n",
    "        # Compute average test loss per batch\n",
    "        avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "        print(\"Average test loss per batch: {:.6f}\".format(avg_test_loss))\n",
    "        print()\n",
    "        \n",
    "        # Compute test accuracy across all batches in this test epoch\n",
    "        print(\"Average test accuracy per batch: {:.2f}% ({:0d}/{:0d})\".format((np.sum(class_correctcount)/np.sum(class_totalcount))*100,\n",
    "                                                                      np.sum(class_correctcount),\n",
    "                                                                      np.sum(class_totalcount)))\n",
    "        print()\n",
    "        \n",
    "        # Compute test accuracy per class\n",
    "        for i in range(len(classes)):\n",
    "            if class_correctcount[i] > 0:\n",
    "                print(\"Test accuracy of {}: {:.2f}% ({:0d}/{:0d})\".format(classes[i],\n",
    "                                                                   (class_correctcount[i]/class_totalcount[i])*100,\n",
    "                                                                   class_correctcount[i],\n",
    "                                                                   class_totalcount[i]))\n",
    "            else:\n",
    "                print(\"Test accuracy of {}: N/A (no training examples)\".format(classes[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3: Check accuracy of model before training\n",
    "\n",
    "Before we proceed to train our model, it is interesting to check out the accuracy of our CNN. Since the parameters of our CNN are randomly generated, we should expect the accuracy to be around 1/10 (i.e. 10%), implying that the prediction of our untrained model is as good as a random guess!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss per batch: 0.002486\n",
      "\n",
      "Average test accuracy per batch: 85.69% (8569/10000)\n",
      "\n",
      "Test accuracy of T-shirt/top: 79.40% (794/1000)\n",
      "Test accuracy of Trouser: 96.40% (964/1000)\n",
      "Test accuracy of Pullover: 75.40% (754/1000)\n",
      "Test accuracy of Dress: 83.90% (839/1000)\n",
      "Test accuracy of Coat: 83.20% (832/1000)\n",
      "Test accuracy of Sandal: 94.80% (948/1000)\n",
      "Test accuracy of Shirt: 64.50% (645/1000)\n",
      "Test accuracy of Sneaker: 88.30% (883/1000)\n",
      "Test accuracy of Bag: 95.20% (952/1000)\n",
      "Test accuracy of Ankle boot: 95.80% (958/1000)\n"
     ]
    }
   ],
   "source": [
    "test(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our results above, we see that indeed, the average accuracy of our model's prediction per batch is about 10%! There are even some classes where our model got completely wrong. Bearing this results in mind, we shall now train our model.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Train the model\n",
    "\n",
    "Remember to look at how the training and validation loss decreases over time; if the validation loss ever increases it indicates possible overfitting. (In fact, in the below example, we could have stopped around epoch XXX or so!)\n",
    "\n",
    "We will train it over 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 out of 50\n",
      "Average training loss: 0.011138\n",
      "Average validation loss: 0.010280\n",
      "Validation loss decreased from inf in the previous epoch to 0.010279915332794189 in this epoch.\n",
      "\n",
      "Epoch: 2 out of 50\n",
      "Average training loss: 0.008956\n",
      "Average validation loss: 0.006954\n",
      "Validation loss decreased from 0.010279915332794189 in the previous epoch to 0.006953837347527345 in this epoch.\n",
      "\n",
      "Epoch: 3 out of 50\n",
      "Average training loss: 0.007493\n",
      "Average validation loss: 0.006023\n",
      "Validation loss decreased from 0.006953837347527345 in the previous epoch to 0.006022635584076246 in this epoch.\n",
      "\n",
      "Epoch: 4 out of 50\n",
      "Average training loss: 0.007008\n",
      "Average validation loss: 0.005618\n",
      "Validation loss decreased from 0.006022635584076246 in the previous epoch to 0.0056176936278740566 in this epoch.\n",
      "\n",
      "Epoch: 5 out of 50\n",
      "Average training loss: 0.006733\n",
      "Average validation loss: 0.005302\n",
      "Validation loss decreased from 0.0056176936278740566 in the previous epoch to 0.005302253182977438 in this epoch.\n",
      "\n",
      "Epoch: 6 out of 50\n",
      "Average training loss: 0.006595\n",
      "Average validation loss: 0.005045\n",
      "Validation loss decreased from 0.005302253182977438 in the previous epoch to 0.005044604635487 in this epoch.\n",
      "\n",
      "Epoch: 7 out of 50\n",
      "Average training loss: 0.006401\n",
      "Average validation loss: 0.005037\n",
      "Validation loss decreased from 0.005044604635487 in the previous epoch to 0.005037236697971821 in this epoch.\n",
      "\n",
      "Epoch: 8 out of 50\n",
      "Average training loss: 0.006355\n",
      "Average validation loss: 0.005025\n",
      "Validation loss decreased from 0.005037236697971821 in the previous epoch to 0.0050248054663340255 in this epoch.\n",
      "\n",
      "Epoch: 9 out of 50\n",
      "Average training loss: 0.006260\n",
      "Average validation loss: 0.004838\n",
      "Validation loss decreased from 0.0050248054663340255 in the previous epoch to 0.004838158002744119 in this epoch.\n",
      "\n",
      "Epoch: 10 out of 50\n",
      "Average training loss: 0.006150\n",
      "Average validation loss: 0.004648\n",
      "Validation loss decreased from 0.004838158002744119 in the previous epoch to 0.0046477577164769175 in this epoch.\n",
      "\n",
      "Epoch: 11 out of 50\n",
      "Average training loss: 0.006136\n",
      "Average validation loss: 0.004637\n",
      "Validation loss decreased from 0.0046477577164769175 in the previous epoch to 0.004637124257783095 in this epoch.\n",
      "\n",
      "Epoch: 12 out of 50\n",
      "Average training loss: 0.006048\n",
      "Average validation loss: 0.004520\n",
      "Validation loss decreased from 0.004637124257783095 in the previous epoch to 0.00451988140369455 in this epoch.\n",
      "\n",
      "Epoch: 13 out of 50\n",
      "Average training loss: 0.006021\n",
      "Average validation loss: 0.004584\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 14 out of 50\n",
      "Average training loss: 0.005857\n",
      "Average validation loss: 0.004416\n",
      "Validation loss decreased from 0.00451988140369455 in the previous epoch to 0.004415994353592396 in this epoch.\n",
      "\n",
      "Epoch: 15 out of 50\n",
      "Average training loss: 0.005908\n",
      "Average validation loss: 0.004395\n",
      "Validation loss decreased from 0.004415994353592396 in the previous epoch to 0.00439503462612629 in this epoch.\n",
      "\n",
      "Epoch: 16 out of 50\n",
      "Average training loss: 0.005774\n",
      "Average validation loss: 0.004231\n",
      "Validation loss decreased from 0.00439503462612629 in the previous epoch to 0.004230890907347202 in this epoch.\n",
      "\n",
      "Epoch: 17 out of 50\n",
      "Average training loss: 0.005746\n",
      "Average validation loss: 0.004238\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 18 out of 50\n",
      "Average training loss: 0.005745\n",
      "Average validation loss: 0.004168\n",
      "Validation loss decreased from 0.004230890907347202 in the previous epoch to 0.0041684852639834085 in this epoch.\n",
      "\n",
      "Epoch: 19 out of 50\n",
      "Average training loss: 0.005733\n",
      "Average validation loss: 0.004448\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 20 out of 50\n",
      "Average training loss: 0.005644\n",
      "Average validation loss: 0.004140\n",
      "Validation loss decreased from 0.0041684852639834085 in the previous epoch to 0.004139617585887511 in this epoch.\n",
      "\n",
      "Epoch: 21 out of 50\n",
      "Average training loss: 0.005655\n",
      "Average validation loss: 0.004006\n",
      "Validation loss decreased from 0.004139617585887511 in the previous epoch to 0.0040064117982983585 in this epoch.\n",
      "\n",
      "Epoch: 22 out of 50\n",
      "Average training loss: 0.005587\n",
      "Average validation loss: 0.004040\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 23 out of 50\n",
      "Average training loss: 0.005470\n",
      "Average validation loss: 0.004082\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 24 out of 50\n",
      "Average training loss: 0.005484\n",
      "Average validation loss: 0.003871\n",
      "Validation loss decreased from 0.0040064117982983585 in the previous epoch to 0.0038707268262902898 in this epoch.\n",
      "\n",
      "Epoch: 25 out of 50\n",
      "Average training loss: 0.005451\n",
      "Average validation loss: 0.003994\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 26 out of 50\n",
      "Average training loss: 0.005477\n",
      "Average validation loss: 0.003865\n",
      "Validation loss decreased from 0.0038707268262902898 in the previous epoch to 0.0038648572998742263 in this epoch.\n",
      "\n",
      "Epoch: 27 out of 50\n",
      "Average training loss: 0.005440\n",
      "Average validation loss: 0.003837\n",
      "Validation loss decreased from 0.0038648572998742263 in the previous epoch to 0.0038372409467895825 in this epoch.\n",
      "\n",
      "Epoch: 28 out of 50\n",
      "Average training loss: 0.005391\n",
      "Average validation loss: 0.004132\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 29 out of 50\n",
      "Average training loss: 0.005328\n",
      "Average validation loss: 0.003737\n",
      "Validation loss decreased from 0.0038372409467895825 in the previous epoch to 0.0037368814883132775 in this epoch.\n",
      "\n",
      "Epoch: 30 out of 50\n",
      "Average training loss: 0.005403\n",
      "Average validation loss: 0.003826\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 31 out of 50\n",
      "Average training loss: 0.005031\n",
      "Average validation loss: 0.003026\n",
      "Validation loss decreased from 0.0037368814883132775 in the previous epoch to 0.0030261754871656497 in this epoch.\n",
      "\n",
      "Epoch: 32 out of 50\n",
      "Average training loss: 0.004636\n",
      "Average validation loss: 0.002838\n",
      "Validation loss decreased from 0.0030261754871656497 in the previous epoch to 0.002837686672185858 in this epoch.\n",
      "\n",
      "Epoch: 33 out of 50\n",
      "Average training loss: 0.004629\n",
      "Average validation loss: 0.002901\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 34 out of 50\n",
      "Average training loss: 0.004518\n",
      "Average validation loss: 0.002749\n",
      "Validation loss decreased from 0.002837686672185858 in the previous epoch to 0.002749249623467525 in this epoch.\n",
      "\n",
      "Epoch: 35 out of 50\n",
      "Average training loss: 0.004496\n",
      "Average validation loss: 0.002540\n",
      "Validation loss decreased from 0.002749249623467525 in the previous epoch to 0.0025404925576100745 in this epoch.\n",
      "\n",
      "Epoch: 36 out of 50\n",
      "Average training loss: 0.004417\n",
      "Average validation loss: 0.002781\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 37 out of 50\n",
      "Average training loss: 0.004508\n",
      "Average validation loss: 0.002704\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 38 out of 50\n",
      "Average training loss: 0.004364\n",
      "Average validation loss: 0.002786\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 39 out of 50\n",
      "Average training loss: 0.004327\n",
      "Average validation loss: 0.002617\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 40 out of 50\n",
      "Average training loss: 0.004314\n",
      "Average validation loss: 0.002519\n",
      "Validation loss decreased from 0.0025404925576100745 in the previous epoch to 0.0025194826393077773 in this epoch.\n",
      "\n",
      "Epoch: 41 out of 50\n",
      "Average training loss: 0.004341\n",
      "Average validation loss: 0.002589\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 42 out of 50\n",
      "Average training loss: 0.004242\n",
      "Average validation loss: 0.002671\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 43 out of 50\n",
      "Average training loss: 0.004278\n",
      "Average validation loss: 0.002607\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 44 out of 50\n",
      "Average training loss: 0.004240\n",
      "Average validation loss: 0.002522\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 45 out of 50\n",
      "Average training loss: 0.004203\n",
      "Average validation loss: 0.002486\n",
      "Validation loss decreased from 0.0025194826393077773 in the previous epoch to 0.0024860059296091396 in this epoch.\n",
      "\n",
      "Epoch: 46 out of 50\n",
      "Average training loss: 0.004224\n",
      "Average validation loss: 0.002566\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 47 out of 50\n",
      "Average training loss: 0.004243\n",
      "Average validation loss: 0.002425\n",
      "Validation loss decreased from 0.0024860059296091396 in the previous epoch to 0.0024252190409849088 in this epoch.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 out of 50\n",
      "Average training loss: 0.004171\n",
      "Average validation loss: 0.002362\n",
      "Validation loss decreased from 0.0024252190409849088 in the previous epoch to 0.002361832876379291 in this epoch.\n",
      "\n",
      "Epoch: 49 out of 50\n",
      "Average training loss: 0.004125\n",
      "Average validation loss: 0.002364\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n",
      "Epoch: 50 out of 50\n",
      "Average training loss: 0.004126\n",
      "Average validation loss: 0.002362\n",
      "Validation loss did not decrease in this epoch.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_loss_mem, validation_loss_mem = train(train_loader, validation_loader, n_epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Test the model\n",
    "\n",
    "Now that we have trained our model, we can test our model and hopefully get a better prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss per batch: 0.002486\n",
      "\n",
      "Average test accuracy per batch: 85.69% (8569/10000)\n",
      "\n",
      "Test accuracy of T-shirt/top: 79.40% (794/1000)\n",
      "Test accuracy of Trouser: 96.40% (964/1000)\n",
      "Test accuracy of Pullover: 75.40% (754/1000)\n",
      "Test accuracy of Dress: 83.90% (839/1000)\n",
      "Test accuracy of Coat: 83.20% (832/1000)\n",
      "Test accuracy of Sandal: 94.80% (948/1000)\n",
      "Test accuracy of Shirt: 64.50% (645/1000)\n",
      "Test accuracy of Sneaker: 88.30% (883/1000)\n",
      "Test accuracy of Bag: 95.20% (952/1000)\n",
      "Test accuracy of Ankle boot: 95.80% (958/1000)\n"
     ]
    }
   ],
   "source": [
    "test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
