{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and Testing Our CNN\n",
    "\n",
    "In the first notebook, we downloaded and prepared the Fashion-MNIST data to be used as input for our CNN.\n",
    "\n",
    "In the second notebook, we designed the architecture of the CNN, defining the feedforward layers, the loss function and the optimisation (backpropagation) method.\n",
    "\n",
    "In this notebook, we will train our data on the `train_set` (with the help of our `validation_set`) and also test the accuracy of our model on the `test_set`.\n",
    "\n",
    "### Outline of this notebook\n",
    "\n",
    ">1. Preliminaries for training\n",
    "    - 1.1: `train` function\n",
    "    - 1.2: `test` function\n",
    "    - 1.3: Check accuracy of model before training\n",
    "<br>\n",
    "<br>\n",
    "2. Train the model\n",
    "<br>\n",
    "<br>\n",
    "3. Test the model\n",
    "\n",
    "We first load the data and define the CNN. Refer to the first two notebooks for detailed walkthroughs of the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#### Load the data\n",
    "\n",
    "train_set = datasets.FashionMNIST(root = 'FashionMNIST_data', train = True, download = True)\n",
    "test_set = datasets.FashionMNIST(root = 'FashionMNIST_data', train = False, download = True)\n",
    "\n",
    "validation = 0.2\n",
    "\n",
    "training_size = len(train_set)\n",
    "indices = list(range(training_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(validation * training_size))\n",
    "train_index = indices[:split]\n",
    "validation_index = indices[split:]\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "validation_sampler = SubsetRandomSampler(validation_index)\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, ), (0.5, ))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, ), (0.5, ))])\n",
    "\n",
    "train_set = datasets.FashionMNIST(root='./FashionMNIST_data', train=True, download=False, transform=train_transform)\n",
    "test_set = datasets.FashionMNIST(root='./FashionMNIST_data', train=False, download=False, transform=test_transform)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                           batch_size = batch_size,\n",
    "                                           sampler = train_sampler, \n",
    "                                           shuffle = False)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                                batch_size = batch_size, \n",
    "                                                sampler = validation_sampler, \n",
    "                                                shuffle = False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = True)\n",
    "\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "#### Design the CNN architecture\n",
    "\n",
    "num_filters = 10\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layer 1 (sees 28x28x1 image tensor)\n",
    "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size, padding = padding)\n",
    "        # Convolutional layer 2 (sees a 14x14x10 tensor)\n",
    "        self.conv2 = nn.Conv2d(14, 20, kernel_size, padding = padding)\n",
    "        # Maxpooling layer of size 2x2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Fully-connected linear layer 1 (sees a 7x7x20 tensor -> 300)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 20, 300)\n",
    "        # Fully-connected linear layer 2 (300 -> 10)\n",
    "        self.fc2 = nn.Linear(300, 10)\n",
    "        # Dropout layer (p=0.2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Design sequence of convolutional and pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten image input into a row vector to feed into the fully-connected layers\n",
    "        x = x.view(-1, 7 * 7 * 20)\n",
    "        # Add fully-connected layer 1 with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        # Add fully-connected layer 2 with dropout\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        return x\n",
    "    \n",
    "model = CNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Prelimiaries for training\n",
    "\n",
    "#### 1.1: Define the `train` function\n",
    "\n",
    "We start by defining a `train` function that will be called when we train our CNN. We will train over a number of `epochs`, which is the number of times that we will cycle through the entire `train_set`.\n",
    "\n",
    "The sequence of steps that is performed for each epoch is:\n",
    ">1. Train with the `train_set`:\n",
    "    - Step 1: Zero's the gradients to prepare for feedforward\n",
    "    - Step 2: Feedforward the input image through the network\n",
    "    - Step 3: Compute the loss relative to `label`\n",
    "    - Step 4: Propagate gradients back into the network's parameters (i.e. calculate the gradient with of the loss function with respect to each parameter)\n",
    "    - Step 5: Optimise / update the parameters\n",
    "    - Step 6: Sum the `training_loss`\n",
    "<br>\n",
    "<br>\n",
    "2. Evaluate with the `validation_set`:\n",
    "    - Step 1: Feedforward the image through the trained model\n",
    "    - Step 2: Compute the loss\n",
    "    - Step 3: Sum the `validation_loss`\n",
    "<br>\n",
    "<br>\n",
    "3. Then, after each epoch, we print the average `training_loss` and `validation_loss` for that epoch.\n",
    "\n",
    "Additionally, from one epoch to another, we keep track of the average validation loss. We save the model as `CNN_FashionMNIST.pt` each time our model attains a new minimum average validation loss for an epoch.\n",
    "\n",
    "Also note that we check if CUDA is available on the machine that we are using. If it is available, we are able to train our CNN on the machine's GPU, else the training is done on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define train function\n",
    "\n",
    "def train(n_epochs):\n",
    "    \n",
    "    # Set minimum validation loss as infinity.\n",
    "    # This is used to check and save the model.\n",
    "    validation_loss_min = np.Inf\n",
    "    \n",
    "    # Iterate over train_set for n_epochs times\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Keep track of training and validation loss\n",
    "        total_training_loss = 0\n",
    "        total_validation_loss = 0\n",
    "        \n",
    "        # ----- Train the model ------\n",
    "        model.train()\n",
    "        \n",
    "        for image, label in train_loader:\n",
    "            # Move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                image, label = image.cuda(), label.cuda()\n",
    "            # Step 1: Zero the gradients\n",
    "            optimiser.zero_grad()\n",
    "            # Step 2: Feedforward the input image\n",
    "            output = model(image)\n",
    "            # Step 3: Compute the loss\n",
    "            loss = criterion(output, label)\n",
    "            # Step 4: Backpropagation\n",
    "            loss.backward()\n",
    "            # Step 5: Update parameters\n",
    "            optimiser.step()\n",
    "            # Step 6: Update training loss\n",
    "            total_training_loss += loss.item()\n",
    "\n",
    "        # ----- Validate the model ------\n",
    "        \n",
    "        # Turn off gradients for validation\n",
    "        model.eval()\n",
    "        \n",
    "        for image, label in validation_loader:\n",
    "            # Move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                image, label = image.cuda(), label.cuda()\n",
    "            # Step 1: Feedforward: Compute predicted outputs after\n",
    "            # training on the train_set above\n",
    "            output = model(image)\n",
    "            # Step 2: Compute loss\n",
    "            loss = criterion(output, label)\n",
    "            # Step 3: Update validation loss\n",
    "            total_validation_loss += loss.item()\n",
    "            \n",
    "        # Compute average losses in this epoch\n",
    "        avg_training_loss = total_training_loss / len(train_loader.sampler)\n",
    "        avg_validation_loss = total_validation_loss / len(validation_loader.sampler)\n",
    "        \n",
    "        # Print average training and validation loss in this epoch\n",
    "        print(\"Epoch: {} out of {}\".format(epoch, n_epochs))\n",
    "        print(\"Average training loss: {:.6f}\".format(avg_training_loss))\n",
    "        print(\"Average validation loss: {:.6f}\".format(avg_validation_loss))\n",
    "        \n",
    "        # ------ Save model if validation loss has decreased ------\n",
    "        if avg_validation_loss <= validation_loss_min:\n",
    "            print(\"Validation loss decreased from {} in the previous epoch to {} in this epoch.\".format(validation_loss_min, avg_validation_loss))\n",
    "            print()\n",
    "            torch.save(model.state_dict(), 'CNN_FashionMNIST.pt')\n",
    "            validation_loss_min = avg_validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Define the `test` function\n",
    "\n",
    "Below, we define the `test` function that we will call when testing our model on unseen images in the `test_set`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test function\n",
    "\n",
    "def test(test_loader):\n",
    "    \n",
    "    # Contruct dictionaries to count the number of correct and\n",
    "    # incorrect predictions for each class\n",
    "    correct_dict = dict(zip(classes, [0]*len(classes)))\n",
    "    incorrect_dict = dict(zip(classes, [0]*len(classes)))\n",
    "    \n",
    "    # Track test loss\n",
    "    total_test_loss = 0\n",
    "    \n",
    "    # Load the saved model with the lowest validation loss\n",
    "    model.load_state_dict(torch.load('CNN_FashionMNIST.pt'))\n",
    "    \n",
    "    # ------ Test the model ------\n",
    "    \n",
    "    # Turn off gradients for validation\n",
    "    model.eval()\n",
    "    \n",
    "    for image, label in test_loader:\n",
    "        # Move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            image, label = image.cuda(), label.cuda()\n",
    "        # Step 1: Feedforward\n",
    "        output = model(image)\n",
    "        # Step 2: Compute loss\n",
    "        loss = criterion(output, label)\n",
    "        # Step 3: Update test loss\n",
    "        total_test_loss += loss.item()\n",
    "        # Step 4: Convert output values to log probabilities using logSoftmax\n",
    "        log_probs = F.log_softmax(output, dim = 1)\n",
    "        # Step 5: Convert log probabilities to probabilities\n",
    "        probs = torch.exp(log_probs)\n",
    "        # Step 6: Determine the predicted class\n",
    "        top_prob, top_prob_class = probs.topk(1, dim=1)\n",
    "        if top_prob_class == label[0]:\n",
    "            \n",
    "    \n",
    "    \n",
    "    # Compute average test loss\n",
    "    avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "    print(\"Test loss: {:.6f}\".format(avg_test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3: Check accuracy of model before training\n",
    "\n",
    "Before we proceed to train our model, it is interesting to check out the accuracy of our CNN. Since the parameters of our CNN are randomly generated, we should expect the accuracy to be around 1/10 (i.e. 10%), implying that the prediction of our untrained model is as good as a random guess!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
